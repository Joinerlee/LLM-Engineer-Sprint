{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c242966",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "딥러닝 학습할 때 각 Layer를 거치면서 데이터의 분포가 계속 변한다. \n",
    "\n",
    "##### Problem of Deep Neural Network (DNN)\n",
    "\n",
    "- Q. Layer을 깊게 쌓으면 좋아지는 거 아니야?\n",
    "\n",
    "    - 현실 : 깊어질수록 학습이 오히려 안 되거나, 아주 느려지는 형상 발생\n",
    "    - 원인: 데이터가 네트워크를 통과할수록 그 `분포(Distribution)`가 제멋대로 널뛰기 때문.\n",
    "\n",
    "- P. 기울기 소실과 폭발(Vanishing & Exploding)\n",
    "    \n",
    "    - Layer를 거칠 때마다 입력갑 $x$가 $W$(가중치)가 곱해짐`\n",
    "\n",
    "\n",
    "입력: x (배치 데이터) [batch_size, num_features]\n",
    "\n",
    "1. 배치 평균 계산:\n",
    "   $μ = (1/m) * Σ(x_i)$\n",
    "   `m = batch_size`, 각 feature별로 평균 구하기\n",
    "\n",
    "2. 배치 분산 계산:\n",
    "   $σ² = (1/m) * Σ(x_i - μ)²$\n",
    "   각 feature별로 분산 구하기\n",
    "\n",
    "3. 정규화 (Normalization):\n",
    "   $x̂ = (x - μ) / √(σ² + ε)$\n",
    "   ε(엡실론)은 0으로 나누는 것 방지 (1e-5)\n",
    "\n",
    "4. 스케일 & 시프트:\n",
    "   $y = γ * x̂ + β$\n",
    "   $γ$(gamma)는 scale, $β$(beta)는 shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1cec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, num_features):\n",
    "        # num_features : feature 갯수 (Linear의 out_features)\n",
    "        self.num_features = num_features\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        # 학습 가능한 파라미터\n",
    "        self.gamma = [1.0]*num_features  # Scale (수정!)\n",
    "        self.beta = [0.0]*num_features   # Shift\n",
    "\n",
    "        # Backward를 위한 캐시 변수들\n",
    "        self.x = None\n",
    "        self.x_normalized = None\n",
    "        self.mu = None\n",
    "        self.var = None\n",
    "        self.std = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : Tensor, Shape [batch_size, num_features]\n",
    "        batch_size = len(x)\n",
    "        self.x = x  # 추가!\n",
    "        \n",
    "        # 1. 배치 평균 계산 (각 feature 별로)\n",
    "        self.mu = []\n",
    "        for j in range(self.num_features):  # 수정!\n",
    "            sum_val = 0\n",
    "            for i in range(batch_size):\n",
    "                sum_val += x[i][j]  # 수정!\n",
    "            mu = sum_val / batch_size\n",
    "            self.mu.append(mu)\n",
    "        \n",
    "        # 2. 배치 분산 계산 (각 feature별로)\n",
    "        self.var = []\n",
    "        for j in range(self.num_features):\n",
    "            sum_squared_diff = 0\n",
    "            for i in range(batch_size):\n",
    "                diff = x[i][j] - self.mu[j]\n",
    "                sum_squared_diff += diff ** 2\n",
    "            variance = sum_squared_diff / batch_size\n",
    "            self.var.append(variance)\n",
    "        \n",
    "        # 3. 표준편차 계산\n",
    "        self.std = []\n",
    "        for j in range(self.num_features):\n",
    "            std = math.sqrt(self.var[j] + self.eps)\n",
    "            self.std.append(std)\n",
    "        \n",
    "        # 4. 정규화 (Normalization)\n",
    "        self.x_normalized = []\n",
    "        for i in range(batch_size):\n",
    "            normalized_row = []\n",
    "            for j in range(self.num_features):\n",
    "                x_norm = (x[i][j] - self.mu[j]) / self.std[j]\n",
    "                normalized_row.append(x_norm)\n",
    "            self.x_normalized.append(normalized_row)\n",
    "        \n",
    "        # 5. Scale and Shift\n",
    "        output = []\n",
    "        for i in range(batch_size):\n",
    "            output_row = []\n",
    "            for j in range(self.num_features):\n",
    "                out_val = self.gamma[j] * self.x_normalized[i][j] + self.beta[j]\n",
    "                output_row.append(out_val)\n",
    "            output.append(output_row)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # grad_output : [batch_size, num_features]\n",
    "        batch_size = len(grad_output)\n",
    "        \n",
    "        # 1. gamma, beta에 대한 gradient\n",
    "        self.grad_gamma = []\n",
    "        self.grad_beta = []\n",
    "        \n",
    "        for j in range(self.num_features):\n",
    "            grad_gamma_j = 0\n",
    "            grad_beta_j = 0\n",
    "            for i in range(batch_size):\n",
    "                grad_gamma_j += grad_output[i][j] * self.x_normalized[i][j]\n",
    "                grad_beta_j += grad_output[i][j]\n",
    "            self.grad_gamma.append(grad_gamma_j)\n",
    "            self.grad_beta.append(grad_beta_j)\n",
    "        \n",
    "        # 2. x_normalized에 대한 gradient\n",
    "        grad_x_normalized = []\n",
    "        for i in range(batch_size):\n",
    "            grad_x_norm_row = []\n",
    "            for j in range(self.num_features):\n",
    "                grad_x_norm = grad_output[i][j] * self.gamma[j]\n",
    "                grad_x_norm_row.append(grad_x_norm)\n",
    "            grad_x_normalized.append(grad_x_norm_row)\n",
    "        \n",
    "        # 3. std에 대한 gradient\n",
    "        grad_std = []\n",
    "        for j in range(self.num_features):\n",
    "            grad_std_j = 0\n",
    "            for i in range(batch_size):\n",
    "                grad_std_j += grad_x_normalized[i][j] * (self.x[i][j] - self.mu[j]) * (-1.0 / (self.std[j] ** 2))\n",
    "            grad_std.append(grad_std_j)\n",
    "        \n",
    "        # 4. var에 대한 gradient\n",
    "        grad_var = []\n",
    "        for j in range(self.num_features):\n",
    "            grad_var_j = grad_std[j] * 0.5 / math.sqrt(self.var[j] + self.eps)\n",
    "            grad_var.append(grad_var_j)\n",
    "        \n",
    "        # 5. mu에 대한 gradient\n",
    "        grad_mu = []\n",
    "        for j in range(self.num_features):\n",
    "            grad_mu_j = 0\n",
    "            # std를 통한 gradient\n",
    "            for i in range(batch_size):\n",
    "                grad_mu_j += grad_x_normalized[i][j] * (-1.0 / self.std[j])\n",
    "            # var를 통한 gradient\n",
    "            grad_mu_j += grad_var[j] * (-2.0 / batch_size) * sum(self.x[i][j] - self.mu[j] for i in range(batch_size))\n",
    "            grad_mu.append(grad_mu_j)\n",
    "        \n",
    "        # 6. x에 대한 gradient\n",
    "        grad_x = []\n",
    "        for i in range(batch_size):\n",
    "            grad_x_row = []\n",
    "            for j in range(self.num_features):\n",
    "                # x_normalized를 통한 gradient\n",
    "                grad_x_ij = grad_x_normalized[i][j] / self.std[j]\n",
    "                # var를 통한 gradient\n",
    "                grad_x_ij += grad_var[j] * (2.0 / batch_size) * (self.x[i][j] - self.mu[j])\n",
    "                # mu를 통한 gradient\n",
    "                grad_x_ij += grad_mu[j] / batch_size\n",
    "                grad_x_row.append(grad_x_ij)\n",
    "            grad_x.append(grad_x_row)\n",
    "        \n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c406a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[-1.2247425750014138, -1.2247425750014138], [0.0, 0.0], [1.2247425750014138, 1.2247425750014138]]\n",
      "Grad x: [[-4.592767433309053e-07, -4.5927674327539414e-07], [0.0, 0.0], [4.592767433309053e-07, 4.5927674327539414e-07]]\n",
      "Grad gamma: [0.4898970300005655, 0.4898970300005655]\n",
      "Grad beta: [0.9, 1.2000000000000002]\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "bn = BatchNorm(num_features=2)\n",
    "\n",
    "x = [\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0]\n",
    "]\n",
    "\n",
    "output = bn.forward(x)\n",
    "print(\"Output:\", output)\n",
    "\n",
    "grad_output = [\n",
    "    [0.1, 0.2],\n",
    "    [0.3, 0.4],\n",
    "    [0.5, 0.6]\n",
    "]\n",
    "\n",
    "grad_x = bn.backward(grad_output)\n",
    "print(\"Grad x:\", grad_x)\n",
    "print(\"Grad gamma:\", bn.grad_gamma)\n",
    "print(\"Grad beta:\", bn.grad_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1817ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
